import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
import os
import json
import graphviz
import pandas as pd
from annotated_text import annotated_text

# Load API_KEY
load_dotenv()
openai_api_key = os.getenv("API_KEY")

# Read JSON file containing the prompts
prompts_agents = []
prompts_consensus = []
json_file_path = "./prompts.json"
try:
    with open(json_file_path, "r") as json_file:
        prompts = json.load(json_file)
        prompts_agents = prompts["agents"]
        prompts_consensus = prompts["consensus"]
except:
    st.write("Erreur lors du chargement du fichier contenant les prompts.")

# Tabs
tab1, tab2 = st.tabs(["ğŸ“„ RÃ©sumeur", "ğŸ” Extracteur"])

with tab1:
    # Show title and description
    st.title("ğŸ“„RÃ©sumeur de rapport")
    st.write("Application de synthÃ¨se de rapport de dÃ©fense par IA gÃ©nÃ©rative. Ce systÃ¨me utilise quatre agents basÃ© sur le LLM `GPT-3.5`. Il effectue trois analyses indÃ©pendantes puis rÃ©alise un consensus entre les trois versions obtenues. Vous pouvez paramÃ©trer le systÃ¨me avec diffÃ©rentes versions de *prompts* pour chaque agent.")

    # Create a graphlib graph object for the workflow
    graph = graphviz.Digraph()
    graph.attr(rankdir='LR')
    graph.attr('node', shape='plaintext')
    graph.node("ğŸ“„ Rapport (entrÃ©e)")
    graph.attr('node', shape='circle')
    graph.edge("ğŸ“„ Rapport (entrÃ©e)", "ğŸ¤– Agent Analyseur 1")
    graph.edge("ğŸ“„ Rapport (entrÃ©e)", "ğŸ¤– Agent Analyseur 2")
    graph.edge("ğŸ“„ Rapport (entrÃ©e)", "ğŸ¤– Agent Analyseur 3")
    graph.edge("ğŸ¤– Agent Analyseur 1", "ğŸ¤ Agent Consensus")
    graph.edge("ğŸ¤– Agent Analyseur 2", "ğŸ¤ Agent Consensus")
    graph.edge("ğŸ¤– Agent Analyseur 3", "ğŸ¤ Agent Consensus")
    graph.attr('node', shape='plaintext')
    graph.edge("ğŸ¤ Agent Consensus", "ğŸ“ RÃ©sumÃ© (sortie)")
    st.graphviz_chart(graph)

    # Create an OpenAI client
    client = OpenAI(api_key=openai_api_key)

    # Split into three columns for three selects
    cols = st.columns(3)
    selected_prompts_agents = [None, None, None]

    # Option to select the prompt for each agent
    for i in range(3):
        with cols[i]:
            option = st.selectbox(
                "Agent Analyseur " + str(i + 1),
                (p["name"] for p in prompts_agents),
            )
            for prompt_agent in prompts_agents:
                if prompt_agent["name"] == option:
                    selected_prompts_agents[i] = prompt_agent["prompt"]
                    break

    # Option to select the prompt for consensus agent
    selected_prompt_consensus = None
    option = st.selectbox(
        "Agent Consensus",
        (p["name"] for p in prompts_consensus),
    )
    for prompt_consensus in prompts_consensus:
        if prompt_consensus["name"] == option:
            selected_prompt_consensus = prompt_consensus["prompt"]
            break

    # Create a drag and drop zone for loading report files
    st.markdown("---")
    uploaded_file = st.file_uploader("ğŸ“‚ SÃ©lectionnez un rapport Ã  analyser", key=1, type="TXT")


    # Read and store the content of the file into a string variable
    if uploaded_file is not None and st.button("ğŸš€ Lancer l'analyse"):
        report_content = uploaded_file.read().decode("utf-8")

        # Display the report content as the user's message
        with st.chat_message("user"):
            st.markdown(report_content)

        # Generate responses for each analyzer agents using the OpenAI API
        with st.spinner("ğŸ’¡ Analyse en cours..."):
            results_agents = [None, None, None]
            for i in range(3):
                results_agents[i] = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": selected_prompts_agents[i] + "\n\n" + report_content}],
                    stream=True,
                )
        
        # Display responses and store their actual result contents
        cols = st.columns(3)
        for i in range(3):
            with cols[i]:
                with st.chat_message("assistant", avatar=f":material/counter_{i + 1}:"):
                    results_agents[i] = st.write_stream(results_agents[i])
        
        # Concatenate all results into one string variable
        all_results_agents = ""
        for i in range(3):
            all_results_agents += f"\n\nRÃ©sumÃ© {i + 1} :\n" + results_agents[i]
        
        # Generate response for the consensus agent using the OpenAI API
        results_consensus = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": selected_prompt_consensus + all_results_agents}],
            stream=True,
        )

        # Stream the response to the chat using `st.write_stream`
        with st.chat_message("assistant"):
            results_consensus = st.write_stream(results_consensus)
        
        # Button to download the text file
        st.download_button(
            label="ğŸ“¥ TÃ©lÃ©charger le rÃ©sumÃ©",
            data=results_consensus,
            file_name="resume_rapport.txt",
            mime="text/plain",
        )

with tab2:
    # Show title and description
    st.title("ğŸ” Extracteur d'Ã©vÃ©nements et d'attributs")
    st.write("Application d'extraction d'Ã©vÃ©nements et d'attributs dans un rapport de dÃ©fense par IA gÃ©nÃ©rative. Vous pouvez paramÃ©trer le systÃ¨me avec diffÃ©rentes versions de *prompts*.")
    
    # Create a drag and drop zone for loading report files
    uploaded_file = st.file_uploader("ğŸ“‚ SÃ©lectionnez un rÃ©sumÃ© Ã  traiter", key=2, type="TXT")

    # Read and store the content of the file into a string variable
    if uploaded_file is not None and st.button("ğŸš€ Lancer l'extraction"):
        report_content = uploaded_file.read().decode("utf-8")

        # Afficher le contenu du rapport
        with st.expander("Contenu du rapport"):
            st.text(report_content)

         # Extraire les Ã©vÃ©nements Ã  l'aide d'OpenAI
        with st.spinner("ğŸ’¡ Extraction en cours..."):
            prompt_extraction = (
                f"Extraire les Ã©vÃ©nements et leurs attributs suivants du texte : {report_content}\n"
                "Veuillez fournir la sortie au format JSON valide avec des guillemets doubles. "
                "Format JSON attendu : [{\"type\": \"...\", \"lieu\": \"...\", \"date\": \"...\", \"acteur\": \"...\"}]"
            )
            try:
                extraction_response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt_extraction}],
                    temperature=0,
                )
                extracted_events_json = extraction_response.choices[0].message.content
                extracted_events = json.loads(extracted_events_json)

                # Partie sophistiquÃ©e pour afficher les Ã©vÃ©nements extraits dans un tableau
                st.markdown("### Ã‰vÃ©nements extraits")

                # CrÃ©er une liste de dictionnaires Ã  partir des Ã©vÃ©nements extraits
                events_data = [
                    {
                        "Type d'Ã©vÃ©nement": event["type"],
                        "Lieu": event["lieu"],
                        "Date": event["date"],
                        "Acteur": event["acteur"]
                    }
                    for event in extracted_events
                ]

                # Convertir les Ã©vÃ©nements en dataframe pour un affichage tabulaire
                events_df = pd.DataFrame(events_data)

                # Afficher le tableau avec les Ã©vÃ©nements
                st.table(events_df)

            except Exception as e:
                st.error(f"Erreur lors de l'extraction : {str(e)}")